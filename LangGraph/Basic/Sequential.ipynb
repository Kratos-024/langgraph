{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "806e392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from typing import TypedDict,Optional\n",
    "from  langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from  langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a686509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Q_A(TypedDict):\n",
    "    question: str\n",
    "    answer: Optional[str]\n",
    "    qna:Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b7cd9f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "                          task=\"text-generation\",\n",
    "                      )\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c54831ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_answer(state:Q_A)->Q_A:\n",
    "    prompt = PromptTemplate(template=\"\"\"Answer the question.\n",
    "Question: {question}\n",
    "Answer:\"\"\",validate_template=True,input_variables=['question'])\n",
    "    prompt = prompt.invoke({  'question':state['question']})\n",
    "    result = model.invoke(prompt)\n",
    "    if type(result.content) == str:\n",
    "        state['answer'] = result.content\n",
    "    return state\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "34823a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_qna(state:Q_A)->Q_A:\n",
    " \n",
    "    prompt = PromptTemplate(template=\"\"\"Can u make 5 Q and A from this : {text}\n",
    "Answer:\"\"\",validate_template=True,input_variables=['text'])\n",
    "    prompt = prompt.invoke({\"text\": state[\"answer\"]})\n",
    "\n",
    "    result = model.invoke(prompt)\n",
    "    if type(result.content) == str:\n",
    "        state['qna'] = result.content\n",
    "    return state\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e7f97093",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(Q_A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1c7d2dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1a4818fac10>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.add_node('give_answer',give_answer)\n",
    "graph.add_node('make_qna',make_qna)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5fecc612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1a4818fac10>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.add_edge(START,'give_answer')\n",
    "graph.add_edge('give_answer','make_qna')\n",
    "graph.add_edge('make_qna',END)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6f23514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d283c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = workflow.invoke({\n",
    "    'question':'What is trasnformers'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5b064837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is trasnformers',\n",
       " 'answer': 'Transformers refer to a computer model known as a transformer architecture, which is a type of neural network designed to process and analyze sequential data such as language, speech, and text.\\n\\nThe transformer architecture was first introduced in 2017 by Google and has since become a widely used and influential model in the field of natural language processing (NLP). It is designed to handle the complexity of language by using self-attention mechanisms, which allow it to weigh the importance of different parts of the input sequence when generating the output.\\n\\nThe original transformer model is composed of several layers, including:\\n\\n1. **Encoder**: This layer takes in the input sequence and breaks it down into smaller components, such as tokens or subwords.\\n2. **Decoder**: This layer generates the output sequence based on the encoded input sequence.\\n3. **Self-Attention**: This mechanism allows the model to weigh the importance of different parts of the input sequence when generating the output.\\n4. **Positional Encoding**: This is a technique used to encode the position of each token in the input sequence.\\n\\nThe transformer architecture has several key benefits, including:\\n\\n1. **Parallelization**: The self-attention mechanism allows the model to process the input sequence in parallel, making it much faster than traditional recurrent neural networks (RNNs).\\n2. **Language Understanding**: The transformer model is able to capture complex relationships between different parts of the input sequence, allowing it to achieve state-of-the-art performance on a range of NLP tasks.\\n3. **Flexibility**: The transformer architecture can be used for a wide range of NLP tasks, including machine translation, text classification, and language modeling.\\n\\nIn addition to its applications in NLP, the transformer architecture has also been used in other fields, such as computer vision and speech recognition.\\n\\nThere are also other types of transformers which refer to \\n\\n1.  **Transformers in Power Systems**: Transformers are used in power systems to transfer electrical energy from one circuit to another through electromagnetic induction. They are a crucial component of the electrical grid and are used to step up or step down voltages to match the requirements of different parts of the system.\\n\\n2.  **Transformers in Robotics**: Transformers are also used in robotics to perform tasks such as grasping and manipulation. They are used to transfer power and motion from one part of the robot to another, allowing the robot to perform complex tasks.\\n\\n3.  **Transformers in Audio Signal Processing**: Transformers are also used in audio signal processing to perform tasks such as noise reduction and audio compression. They are',\n",
       " 'qna': 'Here are 5 Q&A based on the given text:\\n\\n**Q1: What is a transformer architecture in computer science?**\\nA1: A transformer architecture is a type of neural network designed to process and analyze sequential data such as language, speech, and text.\\n\\n**Q2: What is the key benefit of the transformer architecture in terms of processing speed?**\\nA2: The self-attention mechanism of the transformer architecture allows it to process the input sequence in parallel, making it much faster than traditional recurrent neural networks (RNNs).\\n\\n**Q3: What is one of the key benefits of the transformer architecture in terms of language understanding?**\\nA3: The transformer model is able to capture complex relationships between different parts of the input sequence, allowing it to achieve state-of-the-art performance on a range of NLP tasks.\\n\\n**Q4: What are some examples of fields where the transformer architecture is used, aside from natural language processing?**\\nA4: The transformer architecture is also used in computer vision, speech recognition, and audio signal processing, as well as in other fields such as robots and electrical power systems.\\n\\n**Q5: What is one of the key components of the original transformer model?**\\nA5: One of the key components of the original transformer model is the **Encoder** layer, which takes in the input sequence and breaks it down into smaller components, such as tokens or subwords.'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66a7663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
